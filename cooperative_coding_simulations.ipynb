{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493b6f66",
   "metadata": {},
   "source": [
    "# Simulating Cooperatively Coding Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import root\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1f628",
   "metadata": {},
   "source": [
    "## 1D network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rate network\n",
    "class IELagRateNetwork(torch.nn.Module):\n",
    "    def __init__(self, N, tau, dt,\n",
    "        n_lag, width):\n",
    "        super(IELagRateNetwork, self).__init__()\n",
    "        self.N = N\n",
    "        self.tau = tau\n",
    "        self.dt = dt\n",
    "        self.n_lag = n_lag  # IE lag in time steps\n",
    "        self.width = width  # Width of target exponential RF\n",
    "\n",
    "        self.state = torch.zeros(N)\n",
    "        self.tot_state_change_old = torch.zeros(2*n_lag, N)\n",
    "        self.tot_rec_input_old = torch.zeros(2*n_lag, N)\n",
    "        self.rec_net_input_old = torch.zeros(2*n_lag, N)\n",
    "        self.tot_rec_input_expFiltered = torch.zeros(N)\n",
    "        self.rec_net_input_expFiltered = torch.zeros(N)\n",
    "        self.gamma_filter = np.exp(-1/n_lag)\n",
    "        self.gamma_filter_longer = np.exp(-1/(n_lag*1))\n",
    "        self.d_state = N\n",
    "\n",
    "        # Initialize network parameters\n",
    "        self.initialize_base_conn()\n",
    "        self.oscillating_state = torch.ones(N)\n",
    "        self.oscillating_state[::2] = -1\n",
    "        self.oscillating_state /= np.sqrt(N)\n",
    "\n",
    "        self.w_in_netE = torch.nn.Parameter(torch.tensor(.001), requires_grad=True)\n",
    "        self.w_in_I = torch.nn.Parameter(torch.tensor(.001), requires_grad=True)\n",
    "        self.w_rec_netE_param = torch.nn.Parameter(torch.tensor(.001), requires_grad=True)\n",
    "        self.w_rec_I_param = torch.nn.Parameter(torch.tensor(.001), requires_grad=True)\n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def tau_lag(self):\n",
    "        return self.n_lag*self.dt\n",
    "\n",
    "    def initialize_base_conn(self):\n",
    "        self.w_max_rec_netE = 0.5\n",
    "        self.w_max_rec_I = -0.5*(1-self.dt/self.tau)\n",
    "        \n",
    "        # Nearest neighbor connections with periodic boundary conditions\n",
    "        self.A_rec = torch.roll(torch.eye(self.N), 1, dims=0) + torch.roll(torch.eye(self.N), -1, dims=0)\n",
    "        self.A_rec_I = self.A_rec.clone() \n",
    "        self.set_width(self.width)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = torch.zeros(self.N)\n",
    "        self.x_states_delayed = torch.zeros(3*self.n_lag+1, self.N)\n",
    "        self.dxdt_nows_hist = torch.zeros(self.n_lag+1, self.N)\n",
    "        self.tot_state_change_old = torch.zeros(2*self.n_lag, self.N)\n",
    "        self.tot_rec_input_old = torch.zeros(2*self.n_lag, self.N)\n",
    "        self.rec_net_input_old = torch.zeros(2*self.n_lag, self.N)\n",
    "        self.tot_rec_input_expFiltered = torch.zeros(self.N)\n",
    "        self.rec_net_input_expFiltered = torch.zeros(self.N)\n",
    "\n",
    "    def set_width(self, width):\n",
    "        self.width = width\n",
    "        self.gamma = np.exp(-1/width)\n",
    "\n",
    "    @property\n",
    "    def w_tot(self):\n",
    "        # Construct w_tot\n",
    "        w_tot = torch.zeros((2*self.N, 2*self.N))\n",
    "\n",
    "        w_tot[:self.N, :self.N] \\\n",
    "            = (1-self.dt/self.tau)*torch.eye(self.N) + (self.dt/self.tau)*self.w_rec_netE*self.A_rec\n",
    "        w_tot[:self.N, self.N:] \\\n",
    "            = -self.w_rec_I*self.A_rec_I\n",
    "        w_tot[self.N:, :self.N] \\\n",
    "            = (-self.dt/self.tau)*torch.eye(self.N) + (self.dt/self.tau)*self.w_rec_netE*self.A_rec\n",
    "        w_tot[self.N:, self.N:] \\\n",
    "            = -self.w_rec_I*self.A_rec_I\n",
    "        return w_tot\n",
    "\n",
    "    def get_eig(self):\n",
    "        w_tot = self.w_tot\n",
    "\n",
    "        eigvals, eigvecs = torch.linalg.eig(w_tot)\n",
    "        # Return sorted from smallest to largest real part\n",
    "        order = torch.argsort(eigvals.real)\n",
    "        eigvals = eigvals[order]\n",
    "        eigvecs = eigvecs[:, order]\n",
    "        return eigvals, eigvecs\n",
    "\n",
    "    # def set_problematic_eigvecs(self):\n",
    "    #     eigvals, eigvecs = self.get_eig()\n",
    "    #     n_problematic = torch.sum(eigvals.real<0)\n",
    "    #     print(f'Found {n_problematic} problematic (=negative) eigenvalues')\n",
    "    #     # n_problematic = torch.sum(torch.abs(eigvals.real)>1+1e-6)\n",
    "    #     if n_problematic>0:\n",
    "    #         self.problematic_eigvals = eigvals[:n_problematic]\n",
    "    #         \"\"\" Here we take only the 'x' part of the eigenvectors,\n",
    "    #             but neglect the 'dx' part (x_state-x_state_prev) \"\"\"\n",
    "    #         self.problematic_eigvecs = eigvecs.real[:, :n_problematic].T\n",
    "    #         # To be sure, normalize eigvecs\n",
    "    #         self.problematic_eigvecs /= torch.linalg.norm(self.problematic_eigvecs, axis=1)[:, np.newaxis]\n",
    "    #     else:\n",
    "    #         self.problematic_eigvals = torch.tensor([])\n",
    "    #         self.problematic_eigvecs = torch.tensor([])\n",
    "\n",
    "\n",
    "    def saturation(self, x, w_max=0.5):\n",
    "        return 2*w_max*(torch.sigmoid(2*x/w_max)-0.5)\n",
    "    \n",
    "    @property\n",
    "    def w_rec_netE(self):\n",
    "        # Parametrize such that w_rec_netE <= 0.5 and its gradient\n",
    "        # diminishes as w_rec_netE approaches 0.5\n",
    "        return self.saturation(self.w_rec_netE_param)\n",
    "    @w_rec_netE.setter\n",
    "    def w_rec_netE(self, w_rec_netE):\n",
    "        self.w_rec_netE_param.data = self.get_param_value(w_rec_netE, w_max=self.w_max_rec_netE)\n",
    "    \n",
    "    @property\n",
    "    def w_rec_I(self):\n",
    "        # Parametrize such that w_rec_I >= -0.5*(1-dt/tau) and its gradient\n",
    "        # diminishes as w_rec_I approaches -0.5*(1-dt/tau)\n",
    "        # w_max = -0.5*(1-self.dt/self.tau)\n",
    "        return self.saturation(self.w_rec_I_param, w_max=self.w_max_rec_I)\n",
    "    @w_rec_I.setter\n",
    "    def w_rec_I(self, w_rec_I):\n",
    "        self.w_rec_I_param.data = self.get_param_value(w_rec_I, w_max=self.w_max_rec_I)\n",
    "\n",
    "    @property\n",
    "    def w_in_E(self):\n",
    "        return self.w_in_netE - self.w_in_I\n",
    "\n",
    "    @property\n",
    "    def w_rec_E(self):\n",
    "        return self.w_rec_netE - self.w_rec_I\n",
    "\n",
    "    @property\n",
    "    def w_rec_netE_mat(self):\n",
    "        return self.w_rec_netE*self.A_rec\n",
    "\n",
    "    @property\n",
    "    def w_rec_I_mat(self):\n",
    "        return self.w_rec_I * self.A_rec_I\n",
    "\n",
    "    def print_weights(self):\n",
    "        print(f'w_in_netE:      {self.w_in_netE.item()}')\n",
    "        print(f'w_in_I:         {self.w_in_I.item()}')\n",
    "        print(f'w_rec_netE:     {self.w_rec_netE.item()}')\n",
    "        print(f'w_rec_I:        {self.w_rec_I.item()}')\n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = {\n",
    "            'w_in_netE': self.w_in_netE.item(),\n",
    "            'w_in_E': self.w_in_E.item(),\n",
    "            'w_in_I': self.w_in_I.item(),\n",
    "            'w_rec_netE': self.w_rec_netE.item(),\n",
    "            'w_rec_netE_param': self.w_rec_netE_param.item(),\n",
    "            'w_rec_E': self.w_rec_E.item(),\n",
    "            'w_rec_I': self.w_rec_I.item(),\n",
    "            'w_rec_I_param': self.w_rec_I_param.item(),\n",
    "            'w_max_rec_netE': self.w_max_rec_netE,\n",
    "            'w_max_rec_I': self.w_max_rec_I,\n",
    "        }\n",
    "        return weights\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.w_in_netE.data = torch.tensor(weights['w_in_netE'])\n",
    "        self.w_in_I.data = torch.tensor(weights['w_in_I'])\n",
    "        self.w_rec_netE_param.data = torch.tensor(weights['w_rec_netE_param'])\n",
    "        self.w_rec_I_param.data = torch.tensor(weights['w_rec_I_param'])\n",
    "        self.sanitize()\n",
    "\n",
    "    def get_param_value(self, w_target, w_max):\n",
    "        # Convert w_target to torch.tensor if necessary\n",
    "        if not isinstance(w_target, torch.Tensor):\n",
    "            w_target = torch.tensor(w_target)\n",
    "        w_param = - (w_max/2) * torch.log( (w_max-w_target) / (w_max+w_target) )\n",
    "        assert torch.isclose(self.saturation(w_param, w_max), w_target), f'For w_param = {w_param}, self.saturation(w_param, w_max) (={self.saturation(w_param, w_max)}) != w_target (={w_target})'\n",
    "        return w_param\n",
    "    \n",
    "    def set_optimal_net_weights(self, width):\n",
    "        gamma = np.exp(-1/width)\n",
    "        w_2 = torch.tensor(1 / (gamma + 1/gamma))\n",
    "        # Inverse of sigmoid function\n",
    "        self.w_rec_netE = w_2\n",
    "        self.w_in_netE.data = 1.*(1.-w_2*2*gamma)\n",
    "\n",
    "    def stabilize(self, x_state_concat):\n",
    "        # Stabilize by subtratcing projection onto oscillating state\n",
    "        for problematic_eigvec in self.problematic_eigvecs:\n",
    "            x_state_concat -= (x_state_concat@problematic_eigvec)*problematic_eigvec\n",
    "        return x_state_concat\n",
    "\n",
    "    def sanitize(self):\n",
    "        # Sanitize weights\n",
    "        with torch.no_grad():\n",
    "            self.w_in_I.data = torch.clip(self.w_in_I.data, None, 0)\n",
    "            self.w_rec_I_param.data = torch.clip(self.w_rec_I_param.data, None, 0)\n",
    "\n",
    "    \n",
    "    def get_state_derivative(self, t_, x_state, x_state_delayed, x_in):\n",
    "        # t_ is not used, only given for better readability\n",
    "        x_state_ = x_state.clone()\n",
    "        dx_state = x_state_-x_state_delayed\n",
    "        x_state_deriv_times_tau \\\n",
    "            = - x_state_ \\\n",
    "              + self.w_in_netE*x_in \\\n",
    "              + self.w_rec_netE_mat@x_state_  \\\n",
    "              - (self.tau/self.tau_lag)*self.w_rec_I_mat@(dx_state)\n",
    "\n",
    "        x_state_deriv = x_state_deriv_times_tau/self.tau\n",
    "        return x_state_deriv\n",
    "    \n",
    "\n",
    "    def midpoint_method_step(self, x_state, x_state_delayed, x_in, dxdt):\n",
    "        # Midpoint method that integrates the ODE\n",
    "        # x_state'=f(x_state, x_state_delayed, x_in) from t to t+dt.\n",
    "        # It relies on self.dxdt_nows_hist, which are\n",
    "        # stored past changes to x_state (oldest first)\n",
    "        t0 = 0 # Does not matter, is just given for readability\n",
    "        dt = self.dt\n",
    "        dxdt_now = dxdt(t0, x_state, x_state_delayed, x_in)\n",
    "        dxdt_mid = dxdt(t0+dt/2, x_state+(dt/2)*dxdt_now, x_state_delayed+(dt/2)*self.dxdt_nows_hist[0], x_in)\n",
    "        x_state_new = x_state + dt*dxdt_mid\n",
    "        self.dxdt_nows_hist = torch.roll(self.dxdt_nows_hist, -1, dims=0)\n",
    "        self.dxdt_nows_hist[-1] = dxdt_now\n",
    "\n",
    "\n",
    "        return x_state_new\n",
    "    \n",
    "    def forward(self, x_in, x_in_delayed, x_state, x_state_delayed, force_stability=False):\n",
    "        # Use midpoint method to integrate state derivative\n",
    "        x_state_new = self.midpoint_method_step(x_state, x_state_delayed, x_in, self.get_state_derivative)\n",
    "\n",
    "        return x_state_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3b4ed",
   "metadata": {},
   "source": [
    "### 1D with SFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776692ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_1D_SFA(d_RF, a_SFA, tau_SFA, N_1D=100, tau=1, dt=0.01, n_steps=1_000, x_init=0., t_pulse_on=None, t_pulse_off=None):\n",
    "    if t_pulse_on==None:\n",
    "        t_pulse_on = n_steps//3\n",
    "    if t_pulse_off==None:\n",
    "        t_pulse_off = 2*n_steps//3\n",
    "    w_rec, w_ff = get_weights(d_RF, a_SFA)\n",
    "    w_rec_mat = np.roll(np.eye(N_1D), 1, axis=0) * w_rec + np.roll(np.eye(N_1D), -1, axis=0) * w_rec\n",
    "\n",
    "    x_state_data = np.zeros((n_steps, N_1D))\n",
    "    u_SFA_data = np.zeros((n_steps, N_1D))\n",
    "    r_in_data = np.zeros((n_steps, N_1D))\n",
    "    loss_data = np.zeros(n_steps)\n",
    "    violation_data = np.zeros(n_steps)\n",
    "\n",
    "    x_state = np.ones(N_1D) * x_init\n",
    "    u_SFA = np.zeros(N_1D)\n",
    "    r_in = np.zeros(N_1D)\n",
    "    x_tar = np.zeros(N_1D)\n",
    "\n",
    "    x_tar_0 = get_target(d_RF, N_1D)\n",
    "    \n",
    "    for i_t in range(n_steps):\n",
    "\n",
    "        # Set input pulse\n",
    "        if i_t==t_pulse_on:\n",
    "            r_in[N_1D//2] = 1\n",
    "            x_tar = np.copy(x_tar_0)\n",
    "\n",
    "        if i_t==t_pulse_off:\n",
    "            r_in[N_1D//2] = 0\n",
    "            x_tar *= 0\n",
    "\n",
    "        # Dynamics\n",
    "        x_state_old = np.copy(x_state)\n",
    "        x_state += (dt/tau) * ( -x_state \n",
    "                               + w_rec_mat@x_state \n",
    "                               + w_ff*r_in \n",
    "                               - a_SFA*u_SFA)\n",
    "        u_SFA += (dt/tau_SFA) * (-u_SFA + x_state_old)\n",
    "\n",
    "        deviation = x_state - x_tar\n",
    "        dev_abs_sum = np.sum(np.abs(deviation))\n",
    "        x_state_data[i_t] = x_state\n",
    "        u_SFA_data[i_t] = u_SFA\n",
    "        r_in_data[i_t] = r_in\n",
    "        loss_data[i_t] = dev_abs_sum\n",
    "        violation_data[i_t] = (dev_abs_sum-np.abs(np.sum(deviation)))/(dev_abs_sum if dev_abs_sum>0 else 1) # Is zero if deviation all have same sign\n",
    "\n",
    "    x_tar_sum = np.sum(np.abs(x_tar_0))\n",
    "    loss_data /= x_tar_sum\n",
    "    t_resp = (np.argmax(loss_data[t_pulse_off:]<np.exp(-1))) \\\n",
    "                * (dt/tau)  # In units of tau\n",
    "\n",
    "    return x_state_data, u_SFA_data, r_in_data, loss_data, t_resp, violation_data\n",
    "\n",
    "def get_weights(d_RF, a_SFA):\n",
    "    gamma = np.exp(-1/d_RF)\n",
    "    w_rec_eff = gamma/(1+gamma**2)\n",
    "    w_ff_eff = (1-gamma**2)/(1+gamma**2)\n",
    "    w_rec = w_rec_eff * (1+a_SFA)\n",
    "    w_ff = w_ff_eff * (1+a_SFA)\n",
    "    return w_rec, w_ff\n",
    "\n",
    "def get_target(d_RF, N_1D):\n",
    "    gamma = np.exp(-1/d_RF)\n",
    "    x_tar = np.minimum(gamma**(N_1D//2-np.arange(N_1D)), gamma**(-N_1D//2+np.arange(N_1D)))\n",
    "    return x_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad78d49",
   "metadata": {},
   "source": [
    "## 2D networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da26dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IELag2DConvNetwork(torch.nn.Module):\n",
    "    def __init__(self, N, tau, dt,\n",
    "        n_lag, width, selectivity='NonlinearMixed'):\n",
    "        super(IELag2DConvNetwork, self).__init__()\n",
    "        self.N = N\n",
    "        self.tau = tau\n",
    "        self.dt = dt\n",
    "        self.n_lag = n_lag  # IE lag in time steps\n",
    "        self.width = width  # Width of target exponential RF\n",
    "        self.selectivity = selectivity\n",
    "        self.dim = 2\n",
    "\n",
    "        self.state = torch.zeros(N, N)\n",
    "        self.d_state = N*N\n",
    "\n",
    "        # Initialize network parameters\n",
    "        self.A_rec = torch.zeros(3,3) # Conv kernel for recurrent connections\n",
    "        self.A_rec[0, 1] = 1 # Bottom neigbor\n",
    "        self.A_rec[1, 0] = 1 # Left neighbor\n",
    "        self.A_rec[1, 2] = 1 # etc.\n",
    "        self.A_rec[2, 1] = 1\n",
    "        \n",
    "\n",
    "        self.set_width(width)\n",
    "\n",
    "        self.w_in_netE = torch.nn.Parameter(torch.tensor(.001), requires_grad=True)\n",
    "        self.w_in_I = torch.nn.Parameter(torch.tensor(-.001), requires_grad=True)\n",
    "        self.w_rec_netE_param = torch.nn.Parameter(torch.tensor(.001), requires_grad=True)\n",
    "        self.w_rec_I_param = torch.nn.Parameter(torch.tensor(-.001), requires_grad=True)\n",
    "\n",
    "        self.w_rec_netE_overwrite = None\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_rec_netE = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, padding_mode='zeros', bias=False)\n",
    "        self.conv_rec_I = torch.nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, padding_mode='zeros', bias=False)\n",
    "\n",
    "        self.set_conv_weights()\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def tau_lag(self):\n",
    "        return self.n_lag*self.dt\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = torch.zeros(self.N, self.N)\n",
    "        self.dxdt_nows_hist = torch.zeros(self.n_lag+1, self.N, self.N)\n",
    "\n",
    "    def pad_state(self, x_state):\n",
    "        x_state_padded = torch.zeros(x_state.shape[0]+2, x_state.shape[1]+2)\n",
    "        x_state_padded[1:-1, 1:-1] = x_state\n",
    "        # Add periodic boundary conditions\n",
    "        x_state_padded[0,1:-1] = x_state[-1,:]\n",
    "        x_state_padded[-1,1:-1] = x_state[0,:]\n",
    "        x_state_padded[1:-1,0] = x_state[:,-1]\n",
    "        x_state_padded[1:-1,-1] = x_state[:,0]\n",
    "        return x_state_padded\n",
    "    \n",
    "    def compute_w_param_grads(self):\n",
    "        self.w_rec_netE_param.grad = 0.25*torch.sum(self.conv_rec_netE.weight.grad * self.A_rec) * self.saturation_deriv(self.w_rec_netE_param, w_max=0.25)\n",
    "        self.w_rec_I_param.grad = 0.25*torch.sum(self.conv_rec_I.weight.grad * self.A_rec) * self.saturation_deriv(self.w_rec_I_param, w_max=-0.25*(1-self.dt/self.tau))\n",
    "\n",
    "    def set_conv_weights(self):\n",
    "        self.conv_rec_netE.weight.data = self.w_rec_netE * self.A_rec[np.newaxis,np.newaxis,:,:]\n",
    "        self.conv_rec_I.weight.data = self.w_rec_I * self.A_rec[np.newaxis,np.newaxis,:,:]\n",
    "\n",
    "    def flatten(self, x):\n",
    "        # If x is 2D, flatten it\n",
    "        if len(x.shape) == 2:\n",
    "            x_flattened = x.flatten()\n",
    "        # If x is 4D, flatten first and last two dimensions\n",
    "        elif len(x.shape) == 4:\n",
    "            x_flattened = x.view(x.shape[0]*x.shape[1], x.shape[2]*x.shape[3])\n",
    "        return x_flattened\n",
    "\n",
    "    def unflatten(self, x):\n",
    "        # If x is 1D, unflatten it\n",
    "        if len(x.shape) == 1:\n",
    "            x_inflattened = x.view(self.N, self.N)\n",
    "        # If x is 2D, unflatten first and last dimension\n",
    "        elif len(x.shape) == 2:\n",
    "            x_inflattened = x.view(self.N, self.N, self.N, self.N)\n",
    "        return x_inflattened\n",
    "\n",
    "    def set_width(self, width):\n",
    "        self.width = width\n",
    "        self.gamma = np.exp(-1/width)\n",
    "        self.gamma_squared = np.exp(-2/width)\n",
    "        assert self.width>0 and self.gamma<1, f'width = {self.width}, gamma = {self.gamma}'\n",
    "\n",
    "        def get_padding_factor(i, j, dim=0):\n",
    "            # i, j range from 0 to self.N-1 + 2\n",
    "            # because state is already padded\n",
    "            dx = np.abs(i-(self.N//2+1))\n",
    "            dy = np.abs(j-(self.N//2+1))\n",
    "            x_amp = self.gamma**(dx-1) # Amplitude contribution at border\n",
    "            y_amp = self.gamma**(dy-1)\n",
    "            if dim==0: # Extend along x\n",
    "                # x direction decays, but y direction does not\n",
    "                factor = (x_amp*self.gamma+y_amp)/(x_amp+y_amp)\n",
    "            elif dim==1: # Extend along y\n",
    "                # y direction decays, but x direction does not\n",
    "                factor = (x_amp+y_amp*self.gamma)/(x_amp+y_amp)\n",
    "            return factor\n",
    "            \n",
    "        self.pad_vector_x = get_padding_factor(0, torch.arange(self.N)+1, dim=0)\n",
    "        self.pad_vector_y = get_padding_factor(torch.arange(self.N)+1, 0, dim=1)\n",
    "        assert torch.all(self.pad_vector_x<=1) and torch.all(self.pad_vector_y<=1), f'max(pad_vector_x) = {torch.max(self.pad_vector_x)}, max(pad_vector_y) = {torch.max(self.pad_vector_y)}'\n",
    "\n",
    "    def saturation(self, x, w_max=0.25):\n",
    "        return 2*w_max*(torch.sigmoid(2*x/w_max)-0.5)\n",
    "\n",
    "    def get_param_value(self, w_target, w_max):\n",
    "        # Convert w_target to torch.tensor if necessary\n",
    "        if not isinstance(w_target, torch.Tensor):\n",
    "            w_target = torch.tensor(w_target)\n",
    "        w_param = - (w_max/2) * torch.log( (w_max-w_target) / (w_max+w_target) )\n",
    "        assert torch.isclose(self.saturation(w_param, w_max), w_target), f'For w_param = {w_param}, self.saturation(w_param, w_max) (={self.saturation(w_param, w_max)}) != w_target (={w_target})'\n",
    "        return w_param\n",
    "    \n",
    "    def saturation_deriv(self, x, w_max=0.25):\n",
    "        return 4*torch.sigmoid(2*x/w_max)*(1-torch.sigmoid(2*x/w_max))\n",
    "    \n",
    "    @property\n",
    "    def w_rec_netE(self):\n",
    "        # Parametrize such that w_rec_netE <= 0.25 and its gradient\n",
    "        # diminishes as w_rec_netE approaches 0.25\n",
    "        if self.w_rec_netE_overwrite is not None:\n",
    "            return self.w_rec_netE_overwrite\n",
    "        else:\n",
    "            return self.saturation(self.w_rec_netE_param, w_max=0.25)\n",
    "    @w_rec_netE.setter\n",
    "    def w_rec_netE(self, w_rec_netE):\n",
    "        self.w_rec_netE_param.data = self.get_param_value(w_rec_netE, w_max=torch.tensor(0.25))\n",
    "        self.set_conv_weights()\n",
    "    \n",
    "    @property\n",
    "    def w_rec_I(self):\n",
    "        # Parametrize such that w_rec_I >= -0.25*(1-dt/tau) and its gradient\n",
    "        # diminishes as w_rec_I approaches -0.25*(1-dt/tau)\n",
    "        w_max = -0.25\n",
    "        return self.saturation(self.w_rec_I_param, w_max=w_max)\n",
    "    @w_rec_I.setter\n",
    "    def w_rec_I(self, w_rec_I):\n",
    "        w_max = -0.25\n",
    "        self.w_rec_I_param.data = self.get_param_value(w_rec_I, w_max=w_max)\n",
    "        self.set_conv_weights()\n",
    "\n",
    "    @property\n",
    "    def w_in_E(self):\n",
    "        return self.w_in_netE - self.w_in_I\n",
    "\n",
    "    @property\n",
    "    def w_rec_E(self):\n",
    "        return self.w_rec_netE - self.w_rec_I\n",
    "\n",
    "    @property\n",
    "    def w_rec_netE_mat(self):\n",
    "        return self.w_rec_netE*self.A_rec\n",
    "\n",
    "    @property\n",
    "    def w_rec_I_mat(self):\n",
    "        return self.w_rec_I * self.A_rec\n",
    "\n",
    "    def print_weights(self):\n",
    "        print(f'w_in_netE:      {self.w_in_netE.item()}')\n",
    "        print(f'w_in_I:         {self.w_in_I.item()}')\n",
    "        print(f'w_rec_netE:     {self.w_rec_netE.item()}')\n",
    "        print(f'w_rec_I:        {self.w_rec_I.item()}')\n",
    "\n",
    "    def get_weights(self):\n",
    "        weights = {\n",
    "            'w_in_netE': self.w_in_netE.item(),\n",
    "            'w_in_E': self.w_in_E.item(),\n",
    "            'w_in_I': self.w_in_I.item(),\n",
    "            'w_rec_netE': self.w_rec_netE.item(),\n",
    "            'w_rec_netE_param': self.w_rec_netE_param.item(),\n",
    "            'w_rec_E': self.w_rec_E.item(),\n",
    "            'w_rec_I': self.w_rec_I.item(),\n",
    "            'w_rec_I_param': self.w_rec_I_param.item(),\n",
    "        }\n",
    "        return weights\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        # # Should not do anything, but seems necessary\n",
    "        # self.w_rec_netE = torch.tensor(weights['w_rec_netE'])\n",
    "        # self.w_rec_I = torch.tensor(weights['w_rec_I'])\n",
    "        # Override what was done above (for higher precision)\n",
    "        self.w_in_netE.data = torch.tensor(weights['w_in_netE'])\n",
    "        self.w_in_I.data = torch.tensor(weights['w_in_I'])\n",
    "        self.w_rec_netE_param.data = torch.tensor(weights['w_rec_netE_param'])\n",
    "        self.w_rec_I_param.data = torch.tensor(weights['w_rec_I_param'])\n",
    "        # Update convolutional weights\n",
    "        self.set_conv_weights()\n",
    "    \n",
    "    def set_optimal_net_weights(self, width):\n",
    "        gamma = np.exp(-1/width)\n",
    "        w_2 = torch.tensor(0.5 / (gamma + 1/gamma))\n",
    "        # Inverse of sigmoid function\n",
    "        self.w_rec_netE = w_2\n",
    "        self.w_in_netE.data = 1.*(1.-w_2*4*gamma)\n",
    "        self.set_conv_weights()\n",
    "\n",
    "    def sanitize(self):\n",
    "        # Sanitize weights\n",
    "        with torch.no_grad():\n",
    "            self.w_in_I.data = torch.clip(self.w_in_I.data, None, 0)\n",
    "            self.w_rec_I_param.data = torch.clip(self.w_rec_I_param.data, None, 0)\n",
    "            self.set_conv_weights()\n",
    "\n",
    "\n",
    "    def transform_input(self, x_in, x_in_delayed):\n",
    "        if self.selectivity=='LinearMixed':\n",
    "            # In this case x_in[0] is stimulus 1 and x_in[1] is stimulus 2\n",
    "            x_in_ = torch.sum(x_in, dim=1)[:,np.newaxis] + torch.sum(x_in, dim=0)[np.newaxis,:]\n",
    "            x_in_delayed_ = torch.sum(x_in_delayed, dim=1)[:,np.newaxis] + torch.sum(x_in_delayed, dim=0)[np.newaxis,:]\n",
    "        elif self.selectivity=='NonlinearMixed': \n",
    "            x_in_ = x_in\n",
    "            x_in_delayed_ = x_in_delayed\n",
    "        return x_in_, x_in_delayed_\n",
    "\n",
    "    \n",
    "    def get_state_derivative(self, t_, x_state, x_state_delayed, x_in):\n",
    "        # t_ is not used, only given for better readability\n",
    "        x_state_ = x_state.clone()\n",
    "        x_state_padded = self.pad_state(x_state)\n",
    "        dx_state = x_state_-x_state_delayed\n",
    "        dx_state_padded = self.pad_state(dx_state)\n",
    "        x_in_, _ = self.transform_input(x_in, 0*x_in)\n",
    "\n",
    "        x_state_deriv_times_tau \\\n",
    "            = - x_state_ \\\n",
    "              + self.w_in_netE*x_in_ \\\n",
    "              + self.conv_rec_netE(x_state_padded[np.newaxis, :, :])[0,:,:] \\\n",
    "              - (self.tau/self.tau_lag)*self.conv_rec_I(dx_state_padded[np.newaxis, :, :])[0,:,:]\n",
    "\n",
    "        x_state_deriv = x_state_deriv_times_tau/self.tau\n",
    "        return x_state_deriv\n",
    "    \n",
    "\n",
    "    def midpoint_method_step(self, x_state, x_state_delayed, x_in, dxdt):\n",
    "        # Midpoint method that integrates the ODE\n",
    "        # x_state'=f(x_state, x_state_delayed, x_in) from t to t+dt.\n",
    "        # It relies on self.dxdt_nows_hist, which are\n",
    "        # stored past changes to x_state (oldest first)\n",
    "        t0 = 0 # Does not matter, is just given for readability\n",
    "        dt = self.dt\n",
    "        dxdt_now = dxdt(t0, x_state, x_state_delayed, x_in)\n",
    "        dxdt_mid = dxdt(t0+dt/2, x_state+(dt/2)*dxdt_now, x_state_delayed+(dt/2)*self.dxdt_nows_hist[0], x_in)\n",
    "        x_state_new = x_state + dt*dxdt_mid\n",
    "        self.dxdt_nows_hist = torch.roll(self.dxdt_nows_hist, -1, dims=0)\n",
    "        self.dxdt_nows_hist[-1] = dxdt_now\n",
    "\n",
    "        return x_state_new\n",
    "    \n",
    "    def forward(self, x_in, x_in_delayed, x_state, x_state_delayed, force_stability=False):\n",
    "        # Use midpoint method to integrate state derivative\n",
    "        x_state_new = self.midpoint_method_step(x_state, x_state_delayed, x_in, self.get_state_derivative)\n",
    "\n",
    "        return x_state_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803ecd3",
   "metadata": {},
   "source": [
    "## Functions for simulating and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve_net(net, n_steps, x_in, x_target, x_state_init=None, x_in_prev=None, only_loss=False, mask=None):\n",
    "    net.reset()\n",
    "    state_list_len = net.n_lag\n",
    "    if x_state_init is None:\n",
    "        x_state_init = torch.zeros_like(net.state)\n",
    "    states = [x_state_init,]*(state_list_len)\n",
    "    prev_states = states.copy() # No state jumps at t=0\n",
    "    if x_in_prev is None:\n",
    "        x_in_prev = torch.zeros_like(net.state)\n",
    "    x_in_history = [x_in_prev,]*(state_list_len)\n",
    "\n",
    "    loss = 0\n",
    "    loss_curve = np.zeros(n_steps)\n",
    "    violations_x = np.zeros(n_steps)\n",
    "    violations_dx = np.zeros(n_steps)\n",
    "    summed_activity_curve = np.zeros(n_steps)\n",
    "    summed_activity_diff_curve = np.zeros(n_steps)\n",
    "    for t in tqdm(range(n_steps)):\n",
    "        x_in_ = x_in\n",
    "        x_target_ = x_target\n",
    "        x_in_history.append(x_in_)\n",
    "        x_state_new = net(x_in_history[-1], x_in_history[-1-net.n_lag], states[-1], prev_states[-net.n_lag])\n",
    "        loss_bit = error(x_state_new, x_target_) #/ n_steps\n",
    "        loss += loss_bit\n",
    "\n",
    "        prev_states.append(states[-1])\n",
    "        states.append(x_state_new)\n",
    "        loss_curve[t] = loss_bit.item()\n",
    "        summed_activity_curve[t] = torch.sum(x_state_new).detach().numpy()\n",
    "        summed_activity_diff_curve[t] = (torch.sum(x_state_new)-torch.sum(prev_states[-net.n_lag])).detach().numpy()\n",
    "\n",
    "\n",
    "        violations_x[t] = torch.sum(torch.clamp(x_state_new-x_target, 0, None)).detach().numpy()\n",
    "        violations_dx[t] = torch.sum(torch.clamp(x_state_new-prev_states[-1], None, 0)).detach().numpy()\n",
    "\n",
    "        if only_loss:\n",
    "            # Pop oldest elements of prev_states, states, x_in_history\n",
    "            prev_states.pop(0)\n",
    "            states.pop(0)\n",
    "            x_in_history.pop(0)\n",
    "            if t==np.clip(2*n_steps//3, 0, n_steps-1):\n",
    "                dt_twoThirds = t\n",
    "                x_state_twoThirds = x_state_new.detach().clone().numpy()\n",
    "\n",
    "    states = torch.stack(states).detach().numpy()\n",
    "    prev_states = torch.stack(prev_states).detach().numpy()\n",
    "    x_in_history = torch.stack(x_in_history).detach().numpy()\n",
    "    if only_loss:\n",
    "        prev_states = (dt_twoThirds, x_state_twoThirds)\n",
    "    \n",
    "    return loss, loss_curve, states, prev_states, x_in_history, violations_x, violations_dx, summed_activity_curve, summed_activity_diff_curve\n",
    "\n",
    "def error(x, x_target):\n",
    "    return torch.mean(torch.abs((x-x_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb84460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_response_times(net, loss_curve, p_thr_loss, n_powers=1, plot=False):\n",
    "    c_t = net.dt/net.tau\n",
    "\n",
    "    # Define response time as the earliest time step for which the error \n",
    "    # and that of all later time steps is below the threshold.\n",
    "    # Repeat this for powers of the threshold factor p_thr_loss, which\n",
    "    # should correspond to multiples of the decay constants (depending\n",
    "    # on the initialization)\n",
    "    t_loss_reduction_abs_values = np.zeros(n_powers)\n",
    "    for i_pow, n_power in enumerate(range(1, n_powers+1)):\n",
    "        p_thr_loss_pow = p_thr_loss**n_power\n",
    "        loss_threshold_abs = p_thr_loss_pow*loss_curve[0]\n",
    "\n",
    "        is_below_thr_abs = loss_curve<loss_threshold_abs\n",
    "        all_later_below_thr_abs = np.ones_like(loss_curve)\n",
    "        for i in range(len(loss_curve)):\n",
    "            all_later_below_thr_abs[i] = int(np.all(is_below_thr_abs[i:]))\n",
    "        t_loss_reduction_abs = c_t*np.argmax(all_later_below_thr_abs)\n",
    "\n",
    "        t_loss_reduction_abs_values[i_pow] = t_loss_reduction_abs\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "        ax.plot(loss_curve)\n",
    "        ax.axhline(y=loss_threshold_abs, c='gray', ls='dashed')\n",
    "        for i_pow, n_power in enumerate(range(1, n_powers+1)):\n",
    "            ax.axhline(y=(p_thr_loss**n_power)*loss_curve[0], c='gray', ls='dashed')\n",
    "            ax.axvline(x=t_loss_reduction_abs_values[i_pow]/c_t, c='gray', ls='dashed')\n",
    "        ax.set_yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "    if n_powers==1:\n",
    "        t_loss_reduction_abs_values = t_loss_reduction_abs_values[0]\n",
    "\n",
    "    # t_prop = np.mean([np.median(time_deltas_down), np.median(time_deltas_up)])\n",
    "    return t_loss_reduction_abs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tresp_and_wrisc(w_rec_netE_sum, tau, tau_lag):\n",
    "    t_resp_net = tau/(1-w_rec_netE_sum)\n",
    "    def func(t_resp_full_sqrt):\n",
    "        t_resp_full = t_resp_full_sqrt**2\n",
    "        # Return the residual of the equation determining t_resp_full\n",
    "        return (1/t_resp_full) - (  (1/t_resp_net) + (1/tau_lag)*(1-np.exp(-tau_lag/t_resp_full))  )\n",
    "    \n",
    "    # Minimize the residual\n",
    "    res = root(func, 0.001*t_resp_net, args=(), method='hybr', tol=1e-15)\n",
    "\n",
    "    t_resp_full = (res.x**2)[0]\n",
    "    w_rec_I_sum = - np.exp(-tau_lag/t_resp_full)\n",
    "    t_dec_I = tau/(1+w_rec_I_sum)\n",
    "\n",
    "    return t_resp_full, w_rec_I_sum, t_dec_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cecf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1D_RF_gamma(w_rec_netE):\n",
    "    # gamma, the factor of the exponential RF decay, determined by w_rec_netE\n",
    "    gamma = (1/(2*w_rec_netE)) - np.sqrt((1/(2*w_rec_netE))**2 - 1)\n",
    "    return gamma\n",
    "\n",
    "def get_1D_target(w_rec_netE, N_1D):\n",
    "    # Return the target exponential RF with gamma determined by w_rec_netE\n",
    "    gamma = get_1D_RF_gamma(w_rec_netE)\n",
    "    assert np.isclose(1/(gamma+1/gamma), w_rec_netE), \\\n",
    "        f'For gamma = {gamma}, 1/(gamma+1/gamma) = {1/(gamma+1/gamma)} != w_rec_netE = {w_rec_netE}'\n",
    "    return gamma**torch.abs(torch.arange(N_1D)-N_1D//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c164786d",
   "metadata": {},
   "source": [
    "# Running the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_in(N):\n",
    "    x_in = torch.zeros(N)\n",
    "    x_in[N//2] = 1\n",
    "    return x_in\n",
    "\n",
    "def get_x_target(N, width):\n",
    "    x_target = torch.zeros(N)\n",
    "    x_target[N//2] = 1\n",
    "    for i in range(N//2+1, N):\n",
    "        x_target[i] = x_target[i-1]*np.exp(-1/width)\n",
    "    for i in range(N//2-1, -1, -1):\n",
    "        x_target[i] = x_target[i+1]*np.exp(-1/width)\n",
    "    return x_target\n",
    "\n",
    "def get_x_in_2D(N):\n",
    "    x_in = torch.zeros(N, N)\n",
    "    x_in[N//2, N//2] = 1\n",
    "    return x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_1D = 200\n",
    "N_2D = 200\n",
    "tau = 1\n",
    "dt = 0.01\n",
    "dt_vanilla = 0.1\n",
    "n_lag = 10\n",
    "tau_lag = n_lag*dt\n",
    "\n",
    "dimensionality = 2\n",
    "selectivity = 'LinearMixed'\n",
    "isBalanced = False # Set True for balanced, False for excitatory network\n",
    "\n",
    "\n",
    "t_decay = 100 # Target response time of an excitatory network\n",
    "\n",
    "\n",
    "\n",
    "# Set weights and input\n",
    "if dimensionality==1:\n",
    "    x_in = get_x_in(N_1D)\n",
    "    w_rec_netE = 0.5*(1 - tau/t_decay)\n",
    "    wrns = 2* w_rec_netE\n",
    "    t_resp_full, w_rec_I_sum, t_dec_I = get_tresp_and_wrisc(wrns, tau, tau_lag)\n",
    "    w_rec_I = 0.5 * w_rec_I_sum\n",
    "    x_target = get_1D_target(w_rec_netE, N_1D)\n",
    "elif dimensionality==2:\n",
    "    x_in = get_x_in_2D(N_2D)\n",
    "    w_rec_netE = 0.25*(1 - tau/t_decay)\n",
    "    wrns = 4* w_rec_netE\n",
    "    t_resp_full, w_rec_I_sum, t_dec_I = get_tresp_and_wrisc(wrns, tau, tau_lag)\n",
    "    w_rec_I = 0.25 * w_rec_I_sum\n",
    "\n",
    "# Initialize network\n",
    "if dimensionality==1:\n",
    "    net = IELagRateNetwork(N_1D, tau=tau, dt=dt, n_lag=n_lag, width=1)\n",
    "elif dimensionality==2:\n",
    "    net = IELag2DConvNetwork(N_2D, tau=tau, dt=dt, n_lag=n_lag, width=1, selectivity=selectivity)\n",
    "\n",
    "if isBalanced:\n",
    "    net.dt = dt\n",
    "    net.w_in_netE.data = torch.tensor(1.) # Input weight for excitatory input:\n",
    "                                          # adjust later to normalize network\n",
    "                                          # response to a maximum of 1, if \n",
    "                                          # wanted\n",
    "    net.w_in_I.data = torch.tensor(0.)\n",
    "    net.w_rec_netE = torch.tensor(w_rec_netE) # Recurrent net weight\n",
    "    net.w_rec_I = torch.tensor(w_rec_I)   # Strength of balanced interactions\n",
    "else:\n",
    "    net.dt = dt_vanilla\n",
    "    net.w_in_netE.data = torch.tensor(1.) \n",
    "    net.w_in_I.data = torch.tensor(0.)\n",
    "    net.w_rec_netE = torch.tensor(w_rec_netE) # Recurrent net weight\n",
    "    net.w_rec_I_param.data = torch.tensor(0.)\n",
    "\n",
    "net.set_weights(net.get_weights())\n",
    "net.sanitize()\n",
    "net.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611774ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isBalanced:\n",
    "    n_steps = 10*int(t_resp_full/net.dt)\n",
    "else:\n",
    "    n_steps = 10*int(t_decay/net.dt)\n",
    "\n",
    "p_thr_loss = np.exp(-1)  # Threshold factor for loss reduction\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Run once to obtain final state (used as target, make sure n_steps is\n",
    "    # sufficiently large)\n",
    "    loss, loss_curve, states, prev_states, x_in_history, violations_x, violations_dx, summed_activity_curve, summed_activity_diff_curve \\\n",
    "        = evolve_net(net, 2*n_steps, x_in, x_target=0*x_in, x_state_init=0*x_in, x_in_prev=None, only_loss=True)\n",
    "    \n",
    "    \n",
    "    # Obtain target state\n",
    "    x_target = states[-1].copy()\n",
    "\n",
    "    # Normalize input weights and (max value of) target to 1\n",
    "    net.w_in_netE.data /= np.max(x_target)\n",
    "    x_target /= np.max(x_target)\n",
    "\n",
    "    # Run with obtained target to obtain the loss evolution\n",
    "    loss, loss_curve, states, prev_states, x_in_history, violations_x, violations_dx, summed_activity_curve, summed_activity_diff_curve \\\n",
    "        = evolve_net(net, n_steps, x_in, x_target=x_target, x_state_init=0*x_in, x_in_prev=None, only_loss=True)\n",
    "    \n",
    "    # Calculate response time\n",
    "    t_loss_reduction_abs_values = get_loss_response_times(net, loss_curve, p_thr_loss, n_powers=3, plot=True)\n",
    "\n",
    "    \n",
    "print(f'Loss for target: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdeac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve and final state\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "c_t = net.dt/net.tau # Conversion factor for time steps to tau units\n",
    "time_grid = np.arange(len(loss_curve))*c_t\n",
    "axes[0].set_title('Loss curve for target')\n",
    "axes[0].plot(time_grid, loss_curve)\n",
    "axes[0].set_xlabel(r't $(\\tau)$')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_yscale('log')\n",
    "# Plot e^-1 of initial loss threshold and time of threshold crossing\n",
    "axes[0].axhline(y=p_thr_loss*loss_curve[0], color='gray', linestyle='dashed', label=f'Threshold: {p_thr_loss:.2f} * initial loss')\n",
    "axes[0].axvline(x=t_loss_reduction_abs_values[0], color='gray', linestyle='dashed')\n",
    "# Plot analytic response time to check\n",
    "if isBalanced: \n",
    "    t_resp_analytic = t_resp_full\n",
    "else:\n",
    "    t_resp_analytic = t_decay\n",
    "axes[0].axvline(x=t_resp_analytic, color='red', linestyle='dotted', label=f'Analytic response time: {t_resp_analytic:.2f} time steps')\n",
    "\n",
    "axes[1].set_title('Final state and target')\n",
    "if dimensionality==1:\n",
    "    axes[1].plot(states[-1], label='Final state')\n",
    "    axes[1].plot(x_target, label='Target state', linestyle='dashed')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xlabel('Neuron index')\n",
    "    axes[1].set_ylabel('State')\n",
    "elif dimensionality==2:\n",
    "    im = axes[1].imshow(states[-1], cmap='viridis', origin='lower', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Final state')\n",
    "    fig.colorbar(im, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea322a34",
   "metadata": {},
   "source": [
    "## 1D SFA network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1D = 200\n",
    "tau = 1\n",
    "dt = 0.01\n",
    "\n",
    "# Values from supplementary Fig. Aa) - i.e., non-optimal, but illustrative\n",
    "d_RF = 4.5            # RF width, determines n_RF = 2*d_RF + 1\n",
    "a_SFA = 0.09           # Strength of SFA (0 means no SFA)\n",
    "tau_SFA = 10*tau      # Time constant of SFA (here long for illustration)\n",
    "\n",
    "n_steps = 500*int(tau/dt)    \n",
    "t_pulse_on = 100*int(tau/dt)  # Time step at which the input pulse is turned on\n",
    "t_pulse_off = 300*int(tau/dt) # Time step at which the input pulse is turned off\n",
    "\n",
    "x_init = 0.\n",
    "\n",
    "x_state_data, u_SFA_data, r_in_data, loss_data_exc, t_resp, violation_data \\\n",
    "    = simulate_1D_SFA(d_RF=d_RF, a_SFA=a_SFA, tau_SFA=tau_SFA, N_1D=N_1D, tau=tau, dt=dt, n_steps=n_steps, x_init=x_init, t_pulse_on=t_pulse_on, t_pulse_off=t_pulse_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe988d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title('Dynamics')\n",
    "axes[0].plot(x_state_data[:,N_1D//2], label=r'$x_{j_0}$', c='black')\n",
    "axes[0].plot(u_SFA_data[:,N_1D//2], label=r'$u_{j_0}$', c='green')\n",
    "axes[0].plot(r_in_data[:,N_1D//2], label=r'$r_{j_0}$', c='blue')\n",
    "axes[0].set_xlabel(r'Time $(\\tau)$')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('Final activity')\n",
    "neuron_range = np.arange(N_1D//2-20, N_1D//2+21)\n",
    "axes[1].plot(neuron_range, x_state_data[t_pulse_off, neuron_range], label='Final state')\n",
    "axes[1].set_xlabel('Neuron index')\n",
    "axes[1].set_ylabel('Response')\n",
    "\n",
    "axes[2].set_title('Loss evolution')\n",
    "axes[2].plot(loss_data_exc, label='Loss')\n",
    "axes[2].set_xlabel(r'Time $(\\tau)$')\n",
    "axes[2].set_ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade6805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ee6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
